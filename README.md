# AI Code Reviewer

# âœ¨ Features
ðŸ§  Uses a local LLM (via Ollama) to generate intelligent code review suggestions

- ðŸ“¥ Receives pull request events via GitHub webhooks

- ðŸ§¾ Parses diffs and formats prompts with clear context

- ðŸ’¬ Posts inline comments and full reviews back to GitHub

- ðŸ”’ No cloud dependency â€” runs entirely locally or on your serve

Modules:

- app/main.py â€” FastAPI server to receive GitHub events

- app/github_client.py â€” GitHub API wrapper

- app/prompts.py â€” LLM prompt formatter

- app/llm.py â€” Local LLM interface (e.g. Ollama)

- app/commenter.py â€” GitHub inline comment/review poster (optional)

ðŸš€ Quick Start
1. Clone the repo
2. Install dependencies
3. Setup environment variables
4. Start Ollama
5. Run the server
6. Expose your local server (for Github webhooks)
7. Create a webhook on your Github repo

ðŸ§  How It Works
- GitHub sends a webhook when a PR is opened/updated

- FastAPI receives the payload and fetches the diff

- The diff is formatted into a prompt and sent to the local LLM

- The LLM returns a review comment

- (Optional) The agent parses the comment and posts it inline using GitHubâ€™s API